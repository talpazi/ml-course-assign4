{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_DIR = \"17flowers\"      # as in your repo\n",
    "NUM_CLASSES = 17\n",
    "IMAGES_PER_CLASS = 80\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPOCHS_HEAD = 8\n",
    "EPOCHS_FINETUNE = 8\n",
    "\n",
    "LR_HEAD = 1e-3\n",
    "LR_FINETUNE = 1e-5\n",
    "\n",
    "SEEDS = [42, 123] # two runs with different seeds\n",
    "\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "\n",
    "def extract_index(fname):\n",
    "    m = re.search(r\"image_(\\d+)\\.jpg$\", fname)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def build_df(data_dir):\n",
    "    files = sorted([f for f in os.listdir(data_dir) if f.lower().endswith(\".jpg\")])\n",
    "    rows = []\n",
    "    for f in files:\n",
    "        idx = extract_index(f)\n",
    "        if idx is None:\n",
    "            continue\n",
    "        label = (idx - 1) // IMAGES_PER_CLASS  # 0..16\n",
    "        if 0 <= label < NUM_CLASSES:\n",
    "            rows.append((os.path.join(data_dir, f), idx, label))\n",
    "    df = pd.DataFrame(rows, columns=[\"filepath\", \"idx\", \"label\"]).sort_values(\"idx\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = build_df(DATA_DIR)\n",
    "print(\"Images:\", len(df), \" Classes:\", df[\"label\"].nunique())\n",
    "df.head()\n",
    "\n",
    "# --- DATA SPLIT ---\n",
    "def make_split(df, seed):\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=0.50, random_state=seed, stratify=df[\"label\"]\n",
    "    )\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, test_size=0.50, random_state=seed, stratify=temp_df[\"label\"]\n",
    "    )\n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n",
    "splits = {}\n",
    "for seed in SEEDS:\n",
    "    train_df, val_df, test_df = make_split(df, seed)\n",
    "    splits[seed] = (train_df, val_df, test_df)\n",
    "    print(f\"Seed={seed} -> train={len(train_df)} val={len(val_df)} test={len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 15:01:34.592806: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# --- AUGMENTATION + PREPROCESSING ---\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "preprocess = tf.keras.applications.vgg19.preprocess_input\n",
    "\n",
    "augment = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "], name=\"augment\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def make_dataset(df, training=False):\n",
    "    paths = df[\"filepath\"].values\n",
    "    labels = df[\"label\"].values.astype(np.int32)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=len(df), reshuffle_each_iteration=True)\n",
    "\n",
    "    def _map_fn(path, label):\n",
    "        img = load_image(path)\n",
    "        if training:\n",
    "            img = augment(img)\n",
    "        img = preprocess(img)      # IMPORTANT for ImageNet-pretrained VGG19\n",
    "        return img, label\n",
    "\n",
    "    ds = ds.map(_map_fn, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) [ 1 13 10  9  5  4  8  9  4 15]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "seed = SEEDS[0]\n",
    "train_df, val_df, test_df = splits[seed]\n",
    "train_ds = make_dataset(train_df, training=True)\n",
    "\n",
    "batch_imgs, batch_labels = next(iter(train_ds))\n",
    "print(batch_imgs.shape, batch_labels[:10].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 4s 0us/step\n",
      "80150528/80134624 [==============================] - 4s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 17)                8721      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,033,105\n",
      "Trainable params: 8,721\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- MODEL DEFINITION ---\n",
    "\n",
    "def build_vgg19_model(num_classes=NUM_CLASSES):\n",
    "    base = tf.keras.applications.VGG19(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "    )\n",
    "    base.trainable = False  # Phase 1: freeze backbone\n",
    "\n",
    "    inputs = keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "    x = base(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model, base\n",
    "\n",
    "model, base = build_vgg19_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEvalCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_ds):\n",
    "        super().__init__()\n",
    "        self.test_ds = test_ds\n",
    "        self.test_losses = []\n",
    "        self.test_accs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        results = self.model.evaluate(self.test_ds, verbose=0)\n",
    "        # results = [loss, accuracy]\n",
    "        self.test_losses.append(results[0])\n",
    "        self.test_accs.append(results[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_for_split(seed):\n",
    "    train_df, val_df, test_df = splits[seed]\n",
    "    train_ds = make_dataset(train_df, training=True)\n",
    "    val_ds   = make_dataset(val_df, training=False)\n",
    "    test_ds  = make_dataset(test_df, training=False)\n",
    "\n",
    "    model, base = build_vgg19_model()\n",
    "\n",
    "    test_cb = TestEvalCallback(test_ds)\n",
    "    early = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True)\n",
    "\n",
    "    # ---- Phase 1: train head ----\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(LR_HEAD),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    hist1 = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS_HEAD,\n",
    "        callbacks=[test_cb, early],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # ---- Phase 2: fine-tune top layers ----\n",
    "    # Unfreeze last conv block (you can tune this)\n",
    "    base.trainable = True\n",
    "    for layer in base.layers[:-4]:   # keep most frozen, unfreeze last ~4 layers\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(LR_FINETUNE),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    hist2 = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS_FINETUNE,\n",
    "        callbacks=[test_cb, early],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Combine histories\n",
    "    history = {}\n",
    "    for k in hist1.history.keys():\n",
    "        history[k] = hist1.history[k] + hist2.history[k]\n",
    "\n",
    "    # Also attach test history from callback (length = total epochs actually run)\n",
    "    history[\"test_loss\"] = test_cb.test_losses\n",
    "    history[\"test_accuracy\"] = test_cb.test_accs\n",
    "\n",
    "    # Final test evaluation (best weights already restored by EarlyStopping)\n",
    "    final_test = model.evaluate(test_ds, verbose=0)\n",
    "    return model, history, final_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " 5/22 [=====>........................] - ETA: 1:28 - loss: 10.3880 - accuracy: 0.0750"
     ]
    }
   ],
   "source": [
    "seed = SEEDS[0]\n",
    "model, history, final_test = run_training_for_split(seed)\n",
    "print(\"Final test [loss, acc]:\", final_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history, title_prefix=\"VGG19\"):\n",
    "    epochs = range(1, len(history[\"loss\"]) + 1)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(epochs, history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.plot(epochs, history[\"test_accuracy\"], label=\"test_acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{title_prefix} Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Loss (Cross-Entropy)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(epochs, history[\"test_loss\"], label=\"test_loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cross-Entropy Loss\")\n",
    "    plt.title(f\"{title_prefix} Cross-Entropy Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_curves(history, title_prefix=f\"VGG19 (seed={seed})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    model, history, final_test = run_training_for_split(seed)\n",
    "    plot_curves(history, title_prefix=f\"VGG19 (seed={seed})\")\n",
    "    results.append({\"seed\": seed, \"test_loss\": float(final_test[0]), \"test_acc\": float(final_test[1])})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean test acc:\", results_df[\"test_acc\"].mean())\n",
    "print(\"Std  test acc:\", results_df[\"test_acc\"].std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
